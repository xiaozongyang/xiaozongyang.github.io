---
title: 第三章 - 分布式系统
date: 2021-09-27 23:16:15
tags:
    - 读书笔记
    - 存储
    - 分布式
---
1. 数据分布：将数据均匀的分不到多个存储节点
        1. 副本之间的数据一致性
        2. 自动容错，自动检测故障、自动迁移负载
            1. **网络永远不可靠**
2. 分布式存储系统中的异常
    1. 服务器宕机：节点失去内存信息，原因可能是内存错误、服务器停电等
    2. 网络异常
        1. 消息丢失
        2. 消息乱序（UDP）
        3. 网络报数据错误
        4. 网络分区：集群节点被划分为多个区域，每个区域内可以正常通信，区域之间无法通信
    3. 磁盘故障
        1. 磁盘损坏：丢失存储在磁盘上的所有数据
        2. 磁盘数据错误：读出来的内容和写进去的不一致（需要 checksum）
3. 网络异常导致的 RPC 请求结果**三态**：成功、失败、未知
    1. {% asset_img B655C206-512D-4D5A-9156-A5129BC53762.png %}
4. 数据一致性：用户角度，客户端读写操作是否符合某种特性；存储系统角度哦，存储系统的多个副本之间数据是否一致，更新的顺序是否相同
    1. 1. 强一致：如果 Client A 写入一个值，**保证**后续的 Client A, B, C 读取操作**都返回最新值**
    2. 弱一致：如果 Client A 写入一个值，**不保证**后续的 Client A, B, C 读取操作能够返回最新值
    3. 最终一致：如果 Client A 写入一个值，**在一段时间后**，能保证 Client A, B, C 读取操作能返回最新值
5. 分布式存储系统衡量指标
    1. 性能：TPS、RT Average、RT99
    2. 可用性： / 系统正常服务的时间
    3. 一致性
    4. 可扩展性
6. 数据分布策略
    1. 哈希：根据某个特征计算**哈希值**，并与集群中的服务器建立**映射关系**
        1. 很那找到一个散列特性很好的哈希函数，从而会出现数据倾斜问题
            1. 手动拆分：标记热点，将热点数据分拆到多台服务器
            2. 动态调整：将热点数据拆分到多台服务器
        2. 服务器上下线时，**映射关系被打乱，数据需要重新分布**，带来大量的数据迁移
            1. 哈希值与服务器的对应关系作为**元数据**，交给专门的元数据服务器关系，查询时分两步，第一步计算哈希值，根据哈希值查元数据服务器，第二步从元数据服务器获得具体的服务器地址
            2. 一致性哈希
                1. {% asset_img F963B98B-F00D-40A9-9D68-DE61866A6869.png %}
                2. 哈希环位置信息维护
                    1. 每台服务器**只记录前一个节点和后一个节点**的位置信息，O(1) 空间复杂度，O(N)时间复杂度
                    2. 每台服务器维护一个路由表，第 i 元素为编号为 `p+2^i-1` 的后继节点，O(logN)空间复杂度,O(logN) 时间复杂度
                    3. 每台服务器维护**全量**位置信息，O(N)空间复杂度，O(1)时间复杂度
            
    2. 顺序分布：将大表熟**顺序**划分为连续的范围，**每个范围称为一个字表**
        1. {% asset_img E2586DCD-2B85-4DB8-9419-CB0EA133EBAB.png %}
        2. 随之插入和删除，会出现数据分布不均匀，某个子表太大，需要进行分裂，某些子表太小需要合并，从而减少系统中的元数据
7. 数据副本复制
    1. 主备复制(Primary-based protocol)：客户端写主副本，主副本的变更同步到备副本，一般通过操作日志实现
        {% asset_img 49BCC0C1-3782-4EC5-AE76-8210D69F89D1.png %}
    2. 多写复制（Replicated-write protocol）：如 NWR，写入是写 W 个副本，读取是从 R 个副本中读，要求 `W + R > N`
8. 一致性与可用性
    1. CAP {% asset_img 37729371-8F69-413C-A438-2EA14D06FB60.png %}
    2. 自动容错导致 P 必须满足，因此 A 和 P 无法同时满足
9. 容错
    1. 故障检测：租约（Lease）协议，worker 只在租约内才允许提供服务，租约快到期时需要重新申请租约，**需要注意时钟不一致的问题**
    2. 副本打散：避免数据的所有副本分布在同一个机架内
    3. 故障恢复
        1. 单层结构：系统中对每个数据分片维护多个副本
            1. 主从切换
            2. 永久故障时，需要**增加副本并同步数据**
        2. 双层结构：分为存储和服务两层，存储层对数据分配维护多个副本，服务层只有一个服务本提供服务，存储层会将数据持久化写入底层的分布式文件系统，每个数据分片同一时刻只有一个提供服务的节点
            1. 加载内存状态，如索引 
        {% asset_img C33110ED-8AF1-4D03-A1E4-8288ABE6464A.png %}
10. 总控节点不会成为可扩展性瓶颈
    1. 总控节点职责：维护元数据（如数据分布信息）、worker 管理、数据定位、故障检测和恢复、负责均衡、调度，**总体来说数据量很小**
    2. 如果总控节点需要维护文件系统目录树，内存容量可能成为性能瓶颈，解决办法是加一层 Meta Server
        {% asset_img 03D3AAC6-92DD-4869-8CE5-BB95B1C872E5.png %}
    3. 如果总控节点只需要维护分片的位置信息，一般不会成为瓶颈
11. 数据库拆分
    1. 同构系统：分库分表，**扩容不够灵活、自动化程度低**
        {% asset_img 8ABCD502-7559-4F07-9990-D37C6E99D004.png %}
    2. 异构系统：线性可扩、扩缩容成本低
        1. 数据按分片组织，每个分片可以在集群任意一个节点上
        2. 每个节点上维护一部分分片
        3. 同一个分片的副本分布在多个不同的节点上
        {% asset_img 093B36AA-319E-4F7F-B586-59A38CE6E1E9.png %}
12. 分布式协议
    1. 2PC（Two-phase Commit）保证节点之间的**原子性**，用于实现分布式事务，是**阻塞协议**
        {% asset_img BBE1AEC6-E4A9-47E6-8EA3-AF3E683B5E6D.png %}
        - 2PC 面临的故障
            1. 事务参与者发生故障：某个事务参与者一直不响应 → 解决办法，**引入超时时间**
            2. 事务协调者发生故障：需要提供备用协调者
    2. 数据一致性协议 Paxos
        - 保证
            1. **只有一个**节点成为主节点
            2. 最终一定会有一个节点成为主节点，不会没有主节点
        - 执行步骤
            1. 批准（Accept） Proposer 发送 accpet 消息，要求其他节点（acceptor）接受某个提议值，Acceptor 可以接受或拒绝
            2. 确认(Acknowledge) 如果超过一半的 acceptor 接受，则提议生效，Proposer 发送 acknowledge 消息通知所有的 Acceptor 提议生效
            {% asset_img 301DC7EE-2E44-4F28-9E75-5266F5FA7221.png %}
13. 跨机房部署
    1. 集群整体切换（Master 和 Worker 有租约）
        {% asset_img 27B6D102-95D9-41AE-9053-93BEFE10BE2A.png %}
    2. 单个集群跨机房（Master 和 Worker 有租约）
        {% asset_img 259C9107-66CB-40B9-A721-53705DC59955.png %}
    3. Paxos 选主副本（**Master 和 Worker 没有租约**）、工程复杂度太高
        {% asset_img E6588A57-DDB5-4AFF-9852-F9091C008D95.png %}
